{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started with Apache Spark - Part 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is Part-3 of the series \"Start your Journey with Apache Spark\". Please read Part-1 and Part-2 if you havenâ€™t got a chance yet. In this part, we will discuss some of the Advanced DataFrame operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.functions import broadcast\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "emp = [(1, \"AAA\", \"dept1\", 1000),\n",
    "    (2, \"BBB\", \"dept1\", 1100),\n",
    "    (3, \"CCC\", \"dept1\", 3000),\n",
    "    (4, \"DDD\", \"dept1\", 1500),\n",
    "    (5, \"EEE\", \"dept2\", 8000),\n",
    "    (6, \"FFF\", \"dept2\", 7200),\n",
    "    (7, \"GGG\", \"dept3\", 7100),\n",
    "    (None, None, None, 7500),\n",
    "    (9, \"III\", None, 4500),\n",
    "    (10, None, \"dept5\", 2500)]\n",
    "\n",
    "dept = [(\"dept1\", \"Department - 1\"),\n",
    "        (\"dept2\", \"Department - 2\"),\n",
    "        (\"dept3\", \"Department - 3\"),\n",
    "        (\"dept4\", \"Department - 4\")\n",
    "       ]\n",
    "\n",
    "df = spark.createDataFrame(emp, [\"id\", \"name\", \"dept\", \"salary\"])\n",
    "deptdf = spark.createDataFrame(dept, [\"id\", \"name\"]) \n",
    "\n",
    "# Create Temp Tables\n",
    "df.createOrReplaceTempView(\"empdf\")\n",
    "deptdf.createOrReplaceTempView(\"deptdf\")\n",
    "\n",
    "# Save as HIVE tables.\n",
    "df.write.saveAsTable(\"hive_empdf\", mode = \"overwrite\")\n",
    "deptdf.write.saveAsTable(\"hive_deptdf\", mode = \"overwrite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BroadCast Join"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The size of the broadcast table is 10 MB. However, we can change the threshold upto 8GB as per the official documentation of Spark 2.3.\n",
    "\n",
    "* We can check the size of the broadcast table as follow :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default size of broadcast table is 50.0 MB.\n"
     ]
    }
   ],
   "source": [
    "size = int(spark.conf.get(\"spark.sql.autoBroadcastJoinThreshold\")) / (1024 * 1024)\n",
    "print(\"Default size of broadcast table is {0} MB.\".format(size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* we can set the size of the broadcast table to say 50 MB as follows :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", 50 * 1024 * 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consider that we need to join 2 Dataframes. \n",
    "# small_df : Small DataFrame which can fit into memory and smaller than specified threshold.\n",
    "# big_df : Large DataFrame which needs to be join with small DataFrame.\n",
    "\n",
    "# uncomment the below statement. Specify small and big DataFrame.\n",
    "# join_df = big_df.join(broadcast(small_df), big_df[\"id\"] == small_df[\"id\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Caching\n",
    "We can use cache/persist function to keep the dataframe in-memory. It may improve the performance of your spark application significantly if we cache the data which we need to use very frequently in our application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory Used : True\n",
      "Disk Used : True\n"
     ]
    }
   ],
   "source": [
    "df.cache()\n",
    "df.count()\n",
    "print(\"Memory Used : {0}\".format(df.storageLevel.useMemory))\n",
    "print(\"Disk Used : {0}\".format(df.storageLevel.useDisk))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we use cache function it will use storage level as Memory_Only until Spark 2.0.2. Since Spark 2.1.x it is Memory_and_DISK.\n",
    "\n",
    "However, if we need to specify the various available storage levels we can use persist method. For example, if we need to keep the data in Memory only we can use below snippet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.storagelevel import StorageLevel\n",
    "# Ref: Storage Level: https://spark.apache.org/docs/latest/rdd-programming-guide.html#rdd-persistence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory Used : True\n",
      "Disk Used : True\n"
     ]
    }
   ],
   "source": [
    "deptdf.persist(StorageLevel.MEMORY_ONLY)\n",
    "deptdf.count()\n",
    "print(\"Memory Used : {0}\".format(df.storageLevel.useMemory))\n",
    "print(\"Disk Used : {0}\".format(df.storageLevel.useDisk))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Un-Persist\n",
    "It is also important to un-cache the data when it will no longer be required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: bigint, name: string, dept: string, salary: bigint]"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SQL Expressions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use SQL expression for the manipulation of data as well. We have \"expr\" function and also a variant of a select method as \"selectExpr\" for evaluation of SQL expressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+-----+------+------------+\n",
      "|  id|name| dept|salary|salary_level|\n",
      "+----+----+-----+------+------------+\n",
      "|   1| AAA|dept1|  1000|  low_salary|\n",
      "|   2| BBB|dept1|  1100|  low_salary|\n",
      "|   3| CCC|dept1|  3000|  mid_salary|\n",
      "|   4| DDD|dept1|  1500|  low_salary|\n",
      "|   5| EEE|dept2|  8000| high_salary|\n",
      "|   6| FFF|dept2|  7200| high_salary|\n",
      "|   7| GGG|dept3|  7100| high_salary|\n",
      "|null|null| null|  7500| high_salary|\n",
      "|   9| III| null|  4500|  mid_salary|\n",
      "|  10|null|dept5|  2500|  mid_salary|\n",
      "+----+----+-----+------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "\n",
    "# Let's try to categorige the salary under Low, Mid and High as per below categorization.\n",
    "\n",
    "# 0-2000 : low_salary\n",
    "# 2001 - 5000 : mid_salary\n",
    "# > 5001 : high_salary\n",
    "\n",
    "cond = \"\"\"case when salary > 5000 then 'high_salary'\n",
    "               else case when salary > 2000 then 'mid_salary'\n",
    "                    else case when salary > 0 then 'low_salary'\n",
    "                         else 'invalid_salary'\n",
    "                              end\n",
    "                         end\n",
    "                end as salary_level\"\"\"\n",
    "newdf = df.withColumn(\"salary_level\", expr(cond))\n",
    "newdf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using selectExpr function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+-----+------+------------+\n",
      "|  id|name| dept|salary|salary_level|\n",
      "+----+----+-----+------+------------+\n",
      "|   1| AAA|dept1|  1000|  low_salary|\n",
      "|   2| BBB|dept1|  1100|  low_salary|\n",
      "|   3| CCC|dept1|  3000|  mid_salary|\n",
      "|   4| DDD|dept1|  1500|  low_salary|\n",
      "|   5| EEE|dept2|  8000| high_salary|\n",
      "|   6| FFF|dept2|  7200| high_salary|\n",
      "|   7| GGG|dept3|  7100| high_salary|\n",
      "|null|null| null|  7500| high_salary|\n",
      "|   9| III| null|  4500|  mid_salary|\n",
      "|  10|null|dept5|  2500|  mid_salary|\n",
      "+----+----+-----+------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "newdf = df.selectExpr(\"*\", cond)\n",
    "newdf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User Defined Functions (UDF)\n",
    "Often we need to write the function based on the our very specific requirement. Here we can leverage the udfs. We can write our own functions in language like python and register the function as udf, then we can use the function for DataFrame operations.\n",
    "\n",
    "* Python function to find the salary_level for a given salary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detSalary_Level(sal):\n",
    "    level = None\n",
    "\n",
    "    if(sal > 5000):\n",
    "        level = 'high_salary'\n",
    "    elif(sal > 2000):\n",
    "        level = 'mid_salary'\n",
    "    elif(sal > 0):\n",
    "        level = 'low_salary'\n",
    "    else:\n",
    "        level = 'invalid_salary'\n",
    "    return level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Then register the function \"detSalary_Level\" as UDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "sal_level = udf(detSalary_Level, StringType())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Apply function to determine the salary_level for given salary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+-----+------+------------+\n",
      "|  id|name| dept|salary|salary_level|\n",
      "+----+----+-----+------+------------+\n",
      "|   1| AAA|dept1|  1000|  low_salary|\n",
      "|   2| BBB|dept1|  1100|  low_salary|\n",
      "|   3| CCC|dept1|  3000|  mid_salary|\n",
      "|   4| DDD|dept1|  1500|  low_salary|\n",
      "|   5| EEE|dept2|  8000| high_salary|\n",
      "|   6| FFF|dept2|  7200| high_salary|\n",
      "|   7| GGG|dept3|  7100| high_salary|\n",
      "|null|null| null|  7500| high_salary|\n",
      "|   9| III| null|  4500|  mid_salary|\n",
      "|  10|null|dept5|  2500|  mid_salary|\n",
      "+----+----+-----+------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "newdf = df.withColumn(\"salary_level\", sal_level(\"salary\"))\n",
    "newdf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with NULL Values\n",
    "\n",
    "NULL values are always tricky to deal with irrespective of the Framework or language we use. Here in spark we have few specific function to deal with NULL values.\n",
    "\n",
    "- **isNull()**\n",
    "\n",
    "This function will help us to find the null values for any given columns. For e.g. if we need to find the columns where id columns contains the null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+------+\n",
      "|  id|name|dept|salary|\n",
      "+----+----+----+------+\n",
      "|null|null|null|  7500|\n",
      "|   9| III|null|  4500|\n",
      "+----+----+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "newdf = df.filter(df[\"dept\"].isNull())\n",
    "newdf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **isNotNull()**\n",
    "\n",
    "This function works opposite to isNull() function and will return all the not null values for a particular function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+-----+------+\n",
      "| id|name| dept|salary|\n",
      "+---+----+-----+------+\n",
      "|  1| AAA|dept1|  1000|\n",
      "|  2| BBB|dept1|  1100|\n",
      "|  3| CCC|dept1|  3000|\n",
      "|  4| DDD|dept1|  1500|\n",
      "|  5| EEE|dept2|  8000|\n",
      "|  6| FFF|dept2|  7200|\n",
      "|  7| GGG|dept3|  7100|\n",
      "| 10|null|dept5|  2500|\n",
      "+---+----+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "newdf = df.filter(df[\"dept\"].isNotNull())\n",
    "newdf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **fillna()**\n",
    "\n",
    "This function will help us to replace Null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+-------+------+\n",
      "|  id|name|   dept|salary|\n",
      "+----+----+-------+------+\n",
      "|   1| AAA|  dept1|  1000|\n",
      "|   2| BBB|  dept1|  1100|\n",
      "|   3| CCC|  dept1|  3000|\n",
      "|   4| DDD|  dept1|  1500|\n",
      "|   5| EEE|  dept2|  8000|\n",
      "|   6| FFF|  dept2|  7200|\n",
      "|   7| GGG|  dept3|  7100|\n",
      "|null|null|INVALID|  7500|\n",
      "|   9| III|INVALID|  4500|\n",
      "|  10|null|  dept5|  2500|\n",
      "+----+----+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Replace -1 where the salary is null.\n",
    "newdf = df.fillna(\"INVALID\", [\"dept\"])\n",
    "newdf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **dropna()**\n",
    "This function will help us to remove the rows with null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+-----+------+\n",
      "| id|name| dept|salary|\n",
      "+---+----+-----+------+\n",
      "|  1| AAA|dept1|  1000|\n",
      "|  2| BBB|dept1|  1100|\n",
      "|  3| CCC|dept1|  3000|\n",
      "|  4| DDD|dept1|  1500|\n",
      "|  5| EEE|dept2|  8000|\n",
      "|  6| FFF|dept2|  7200|\n",
      "|  7| GGG|dept3|  7100|\n",
      "+---+----+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Remove all rows which contains any null values.\n",
    "newdf = df.dropna()\n",
    "newdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+-----+------+\n",
      "|  id|name| dept|salary|\n",
      "+----+----+-----+------+\n",
      "|   1| AAA|dept1|  1000|\n",
      "|   2| BBB|dept1|  1100|\n",
      "|   3| CCC|dept1|  3000|\n",
      "|   4| DDD|dept1|  1500|\n",
      "|   5| EEE|dept2|  8000|\n",
      "|   6| FFF|dept2|  7200|\n",
      "|   7| GGG|dept3|  7100|\n",
      "|null|null| null|  7500|\n",
      "|   9| III| null|  4500|\n",
      "|  10|null|dept5|  2500|\n",
      "+----+----+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Remove all rows which contains all null values.\n",
    "newdf = df.dropna(how = \"all\")\n",
    "newdf.show()\n",
    "\n",
    "# Note : default value of \"how\" param is \"any\".\n",
    "# Ref : https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.dropna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+-----+------+\n",
      "| id|name| dept|salary|\n",
      "+---+----+-----+------+\n",
      "|  1| AAA|dept1|  1000|\n",
      "|  2| BBB|dept1|  1100|\n",
      "|  3| CCC|dept1|  3000|\n",
      "|  4| DDD|dept1|  1500|\n",
      "|  5| EEE|dept2|  8000|\n",
      "|  6| FFF|dept2|  7200|\n",
      "|  7| GGG|dept3|  7100|\n",
      "| 10|null|dept5|  2500|\n",
      "+---+----+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Remove all rows where columns : dept is null.\n",
    "newdf = df.dropna(subset = \"dept\")\n",
    "newdf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partitioning\n",
    "\n",
    "Partitioning is very important aspect to control the parallelism for spark Application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Check number of partitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Increase number of partitions. For e.g. Increase partitions to 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newdf = df.repartition(6)\n",
    "newdf.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note : This is expensive operations since it require shuffling of data across the workers.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Decrease Number of Partitions. For e.g. decrease partitions to 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newdf = df.coalesce(2)\n",
    "newdf.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* By default the number of partitions for Spark SQL is 200. \n",
    "* But we can eet the Number of partitions at Spark Application level as well. For e.g. set to 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of Partitions : 500\n"
     ]
    }
   ],
   "source": [
    "# Set number of partitions as Spark Application.\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"500\")\n",
    "\n",
    "# Check the number of patitions.\n",
    "num_part = spark.conf.get(\"spark.sql.shuffle.partitions\")\n",
    "print(\"No of Partitions : {0}\".format(num_part))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Catalog API\n",
    "\n",
    "Spark Catalog is a user-facing API, which you can access using SparkSession.catalog."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **listDatabases()**\n",
    "\n",
    "It will return all the databases along with their Location on file system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Database(name='default', description='default database', locationUri='file:/home/jovyan/work/spark-warehouse')]"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listDatabases()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **listTables()**\n",
    "\n",
    "It will return all the tables for a given database along with the information like Table type (External/Managed) and whether a particular table is temporary or permanent.\n",
    "This includes all temporary views."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Table(name='hive_deptdf', database='default', description=None, tableType='MANAGED', isTemporary=False),\n",
       " Table(name='hive_empdf', database='default', description=None, tableType='MANAGED', isTemporary=False),\n",
       " Table(name='deptdf', database=None, description=None, tableType='TEMPORARY', isTemporary=True),\n",
       " Table(name='empdf', database=None, description=None, tableType='TEMPORARY', isTemporary=True)]"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listTables(\"default\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **listColumns()**\n",
    "\n",
    "It will return all the columns for a particular table in DataBase. Also, it will return datatype, if the column is used in Partitioning or Bucketing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Column(name='id', description=None, dataType='bigint', nullable=True, isPartition=False, isBucket=False),\n",
       " Column(name='name', description=None, dataType='string', nullable=True, isPartition=False, isBucket=False),\n",
       " Column(name='dept', description=None, dataType='string', nullable=True, isPartition=False, isBucket=False),\n",
       " Column(name='salary', description=None, dataType='bigint', nullable=True, isPartition=False, isBucket=False)]"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listColumns(\"hive_empdf\", \"default\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **listFunctions()**\n",
    "\n",
    "It will return all the available function in spark Session along with the information whether it is temporary or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Function(name='!', description=None, className='org.apache.spark.sql.catalyst.expressions.Not', isTemporary=True),\n",
       " Function(name='%', description=None, className='org.apache.spark.sql.catalyst.expressions.Remainder', isTemporary=True),\n",
       " Function(name='&', description=None, className='org.apache.spark.sql.catalyst.expressions.BitwiseAnd', isTemporary=True),\n",
       " Function(name='*', description=None, className='org.apache.spark.sql.catalyst.expressions.Multiply', isTemporary=True),\n",
       " Function(name='+', description=None, className='org.apache.spark.sql.catalyst.expressions.Add', isTemporary=True),\n",
       " Function(name='-', description=None, className='org.apache.spark.sql.catalyst.expressions.Subtract', isTemporary=True),\n",
       " Function(name='/', description=None, className='org.apache.spark.sql.catalyst.expressions.Divide', isTemporary=True),\n",
       " Function(name='<', description=None, className='org.apache.spark.sql.catalyst.expressions.LessThan', isTemporary=True),\n",
       " Function(name='<=', description=None, className='org.apache.spark.sql.catalyst.expressions.LessThanOrEqual', isTemporary=True),\n",
       " Function(name='<=>', description=None, className='org.apache.spark.sql.catalyst.expressions.EqualNullSafe', isTemporary=True),\n",
       " Function(name='=', description=None, className='org.apache.spark.sql.catalyst.expressions.EqualTo', isTemporary=True),\n",
       " Function(name='==', description=None, className='org.apache.spark.sql.catalyst.expressions.EqualTo', isTemporary=True),\n",
       " Function(name='>', description=None, className='org.apache.spark.sql.catalyst.expressions.GreaterThan', isTemporary=True),\n",
       " Function(name='>=', description=None, className='org.apache.spark.sql.catalyst.expressions.GreaterThanOrEqual', isTemporary=True),\n",
       " Function(name='^', description=None, className='org.apache.spark.sql.catalyst.expressions.BitwiseXor', isTemporary=True),\n",
       " Function(name='abs', description=None, className='org.apache.spark.sql.catalyst.expressions.Abs', isTemporary=True),\n",
       " Function(name='acos', description=None, className='org.apache.spark.sql.catalyst.expressions.Acos', isTemporary=True),\n",
       " Function(name='add_months', description=None, className='org.apache.spark.sql.catalyst.expressions.AddMonths', isTemporary=True),\n",
       " Function(name='aggregate', description=None, className='org.apache.spark.sql.catalyst.expressions.ArrayAggregate', isTemporary=True),\n",
       " Function(name='and', description=None, className='org.apache.spark.sql.catalyst.expressions.And', isTemporary=True),\n",
       " Function(name='approx_count_distinct', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.HyperLogLogPlusPlus', isTemporary=True),\n",
       " Function(name='approx_percentile', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.ApproximatePercentile', isTemporary=True),\n",
       " Function(name='array', description=None, className='org.apache.spark.sql.catalyst.expressions.CreateArray', isTemporary=True),\n",
       " Function(name='array_contains', description=None, className='org.apache.spark.sql.catalyst.expressions.ArrayContains', isTemporary=True),\n",
       " Function(name='array_distinct', description=None, className='org.apache.spark.sql.catalyst.expressions.ArrayDistinct', isTemporary=True),\n",
       " Function(name='array_except', description=None, className='org.apache.spark.sql.catalyst.expressions.ArrayExcept', isTemporary=True),\n",
       " Function(name='array_intersect', description=None, className='org.apache.spark.sql.catalyst.expressions.ArrayIntersect', isTemporary=True),\n",
       " Function(name='array_join', description=None, className='org.apache.spark.sql.catalyst.expressions.ArrayJoin', isTemporary=True),\n",
       " Function(name='array_max', description=None, className='org.apache.spark.sql.catalyst.expressions.ArrayMax', isTemporary=True),\n",
       " Function(name='array_min', description=None, className='org.apache.spark.sql.catalyst.expressions.ArrayMin', isTemporary=True),\n",
       " Function(name='array_position', description=None, className='org.apache.spark.sql.catalyst.expressions.ArrayPosition', isTemporary=True),\n",
       " Function(name='array_remove', description=None, className='org.apache.spark.sql.catalyst.expressions.ArrayRemove', isTemporary=True),\n",
       " Function(name='array_repeat', description=None, className='org.apache.spark.sql.catalyst.expressions.ArrayRepeat', isTemporary=True),\n",
       " Function(name='array_sort', description=None, className='org.apache.spark.sql.catalyst.expressions.ArraySort', isTemporary=True),\n",
       " Function(name='array_union', description=None, className='org.apache.spark.sql.catalyst.expressions.ArrayUnion', isTemporary=True),\n",
       " Function(name='arrays_overlap', description=None, className='org.apache.spark.sql.catalyst.expressions.ArraysOverlap', isTemporary=True),\n",
       " Function(name='arrays_zip', description=None, className='org.apache.spark.sql.catalyst.expressions.ArraysZip', isTemporary=True),\n",
       " Function(name='ascii', description=None, className='org.apache.spark.sql.catalyst.expressions.Ascii', isTemporary=True),\n",
       " Function(name='asin', description=None, className='org.apache.spark.sql.catalyst.expressions.Asin', isTemporary=True),\n",
       " Function(name='assert_true', description=None, className='org.apache.spark.sql.catalyst.expressions.AssertTrue', isTemporary=True),\n",
       " Function(name='atan', description=None, className='org.apache.spark.sql.catalyst.expressions.Atan', isTemporary=True),\n",
       " Function(name='atan2', description=None, className='org.apache.spark.sql.catalyst.expressions.Atan2', isTemporary=True),\n",
       " Function(name='avg', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.Average', isTemporary=True),\n",
       " Function(name='base64', description=None, className='org.apache.spark.sql.catalyst.expressions.Base64', isTemporary=True),\n",
       " Function(name='bigint', description=None, className='org.apache.spark.sql.catalyst.expressions.Cast', isTemporary=True),\n",
       " Function(name='bin', description=None, className='org.apache.spark.sql.catalyst.expressions.Bin', isTemporary=True),\n",
       " Function(name='binary', description=None, className='org.apache.spark.sql.catalyst.expressions.Cast', isTemporary=True),\n",
       " Function(name='bit_length', description=None, className='org.apache.spark.sql.catalyst.expressions.BitLength', isTemporary=True),\n",
       " Function(name='boolean', description=None, className='org.apache.spark.sql.catalyst.expressions.Cast', isTemporary=True),\n",
       " Function(name='bround', description=None, className='org.apache.spark.sql.catalyst.expressions.BRound', isTemporary=True),\n",
       " Function(name='cardinality', description=None, className='org.apache.spark.sql.catalyst.expressions.Size', isTemporary=True),\n",
       " Function(name='cast', description=None, className='org.apache.spark.sql.catalyst.expressions.Cast', isTemporary=True),\n",
       " Function(name='cbrt', description=None, className='org.apache.spark.sql.catalyst.expressions.Cbrt', isTemporary=True),\n",
       " Function(name='ceil', description=None, className='org.apache.spark.sql.catalyst.expressions.Ceil', isTemporary=True),\n",
       " Function(name='ceiling', description=None, className='org.apache.spark.sql.catalyst.expressions.Ceil', isTemporary=True),\n",
       " Function(name='char', description=None, className='org.apache.spark.sql.catalyst.expressions.Chr', isTemporary=True),\n",
       " Function(name='char_length', description=None, className='org.apache.spark.sql.catalyst.expressions.Length', isTemporary=True),\n",
       " Function(name='character_length', description=None, className='org.apache.spark.sql.catalyst.expressions.Length', isTemporary=True),\n",
       " Function(name='chr', description=None, className='org.apache.spark.sql.catalyst.expressions.Chr', isTemporary=True),\n",
       " Function(name='coalesce', description=None, className='org.apache.spark.sql.catalyst.expressions.Coalesce', isTemporary=True),\n",
       " Function(name='collect_list', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.CollectList', isTemporary=True),\n",
       " Function(name='collect_set', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.CollectSet', isTemporary=True),\n",
       " Function(name='concat', description=None, className='org.apache.spark.sql.catalyst.expressions.Concat', isTemporary=True),\n",
       " Function(name='concat_ws', description=None, className='org.apache.spark.sql.catalyst.expressions.ConcatWs', isTemporary=True),\n",
       " Function(name='conv', description=None, className='org.apache.spark.sql.catalyst.expressions.Conv', isTemporary=True),\n",
       " Function(name='corr', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.Corr', isTemporary=True),\n",
       " Function(name='cos', description=None, className='org.apache.spark.sql.catalyst.expressions.Cos', isTemporary=True),\n",
       " Function(name='cosh', description=None, className='org.apache.spark.sql.catalyst.expressions.Cosh', isTemporary=True),\n",
       " Function(name='cot', description=None, className='org.apache.spark.sql.catalyst.expressions.Cot', isTemporary=True),\n",
       " Function(name='count', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.Count', isTemporary=True),\n",
       " Function(name='count_min_sketch', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.CountMinSketchAgg', isTemporary=True),\n",
       " Function(name='covar_pop', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.CovPopulation', isTemporary=True),\n",
       " Function(name='covar_samp', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.CovSample', isTemporary=True),\n",
       " Function(name='crc32', description=None, className='org.apache.spark.sql.catalyst.expressions.Crc32', isTemporary=True),\n",
       " Function(name='cube', description=None, className='org.apache.spark.sql.catalyst.expressions.Cube', isTemporary=True),\n",
       " Function(name='cume_dist', description=None, className='org.apache.spark.sql.catalyst.expressions.CumeDist', isTemporary=True),\n",
       " Function(name='current_database', description=None, className='org.apache.spark.sql.catalyst.expressions.CurrentDatabase', isTemporary=True),\n",
       " Function(name='current_date', description=None, className='org.apache.spark.sql.catalyst.expressions.CurrentDate', isTemporary=True),\n",
       " Function(name='current_timestamp', description=None, className='org.apache.spark.sql.catalyst.expressions.CurrentTimestamp', isTemporary=True),\n",
       " Function(name='date', description=None, className='org.apache.spark.sql.catalyst.expressions.Cast', isTemporary=True),\n",
       " Function(name='date_add', description=None, className='org.apache.spark.sql.catalyst.expressions.DateAdd', isTemporary=True),\n",
       " Function(name='date_format', description=None, className='org.apache.spark.sql.catalyst.expressions.DateFormatClass', isTemporary=True),\n",
       " Function(name='date_sub', description=None, className='org.apache.spark.sql.catalyst.expressions.DateSub', isTemporary=True),\n",
       " Function(name='date_trunc', description=None, className='org.apache.spark.sql.catalyst.expressions.TruncTimestamp', isTemporary=True),\n",
       " Function(name='datediff', description=None, className='org.apache.spark.sql.catalyst.expressions.DateDiff', isTemporary=True),\n",
       " Function(name='day', description=None, className='org.apache.spark.sql.catalyst.expressions.DayOfMonth', isTemporary=True),\n",
       " Function(name='dayofmonth', description=None, className='org.apache.spark.sql.catalyst.expressions.DayOfMonth', isTemporary=True),\n",
       " Function(name='dayofweek', description=None, className='org.apache.spark.sql.catalyst.expressions.DayOfWeek', isTemporary=True),\n",
       " Function(name='dayofyear', description=None, className='org.apache.spark.sql.catalyst.expressions.DayOfYear', isTemporary=True),\n",
       " Function(name='decimal', description=None, className='org.apache.spark.sql.catalyst.expressions.Cast', isTemporary=True),\n",
       " Function(name='decode', description=None, className='org.apache.spark.sql.catalyst.expressions.Decode', isTemporary=True),\n",
       " Function(name='degrees', description=None, className='org.apache.spark.sql.catalyst.expressions.ToDegrees', isTemporary=True),\n",
       " Function(name='dense_rank', description=None, className='org.apache.spark.sql.catalyst.expressions.DenseRank', isTemporary=True),\n",
       " Function(name='double', description=None, className='org.apache.spark.sql.catalyst.expressions.Cast', isTemporary=True),\n",
       " Function(name='e', description=None, className='org.apache.spark.sql.catalyst.expressions.EulerNumber', isTemporary=True),\n",
       " Function(name='element_at', description=None, className='org.apache.spark.sql.catalyst.expressions.ElementAt', isTemporary=True),\n",
       " Function(name='elt', description=None, className='org.apache.spark.sql.catalyst.expressions.Elt', isTemporary=True),\n",
       " Function(name='encode', description=None, className='org.apache.spark.sql.catalyst.expressions.Encode', isTemporary=True),\n",
       " Function(name='exists', description=None, className='org.apache.spark.sql.catalyst.expressions.ArrayExists', isTemporary=True),\n",
       " Function(name='exp', description=None, className='org.apache.spark.sql.catalyst.expressions.Exp', isTemporary=True),\n",
       " Function(name='explode', description=None, className='org.apache.spark.sql.catalyst.expressions.Explode', isTemporary=True),\n",
       " Function(name='explode_outer', description=None, className='org.apache.spark.sql.catalyst.expressions.Explode', isTemporary=True),\n",
       " Function(name='expm1', description=None, className='org.apache.spark.sql.catalyst.expressions.Expm1', isTemporary=True),\n",
       " Function(name='factorial', description=None, className='org.apache.spark.sql.catalyst.expressions.Factorial', isTemporary=True),\n",
       " Function(name='filter', description=None, className='org.apache.spark.sql.catalyst.expressions.ArrayFilter', isTemporary=True),\n",
       " Function(name='find_in_set', description=None, className='org.apache.spark.sql.catalyst.expressions.FindInSet', isTemporary=True),\n",
       " Function(name='first', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.First', isTemporary=True),\n",
       " Function(name='first_value', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.First', isTemporary=True),\n",
       " Function(name='flatten', description=None, className='org.apache.spark.sql.catalyst.expressions.Flatten', isTemporary=True),\n",
       " Function(name='float', description=None, className='org.apache.spark.sql.catalyst.expressions.Cast', isTemporary=True),\n",
       " Function(name='floor', description=None, className='org.apache.spark.sql.catalyst.expressions.Floor', isTemporary=True),\n",
       " Function(name='format_number', description=None, className='org.apache.spark.sql.catalyst.expressions.FormatNumber', isTemporary=True),\n",
       " Function(name='format_string', description=None, className='org.apache.spark.sql.catalyst.expressions.FormatString', isTemporary=True),\n",
       " Function(name='from_json', description=None, className='org.apache.spark.sql.catalyst.expressions.JsonToStructs', isTemporary=True),\n",
       " Function(name='from_unixtime', description=None, className='org.apache.spark.sql.catalyst.expressions.FromUnixTime', isTemporary=True),\n",
       " Function(name='from_utc_timestamp', description=None, className='org.apache.spark.sql.catalyst.expressions.FromUTCTimestamp', isTemporary=True),\n",
       " Function(name='get_json_object', description=None, className='org.apache.spark.sql.catalyst.expressions.GetJsonObject', isTemporary=True),\n",
       " Function(name='greatest', description=None, className='org.apache.spark.sql.catalyst.expressions.Greatest', isTemporary=True),\n",
       " Function(name='grouping', description=None, className='org.apache.spark.sql.catalyst.expressions.Grouping', isTemporary=True),\n",
       " Function(name='grouping_id', description=None, className='org.apache.spark.sql.catalyst.expressions.GroupingID', isTemporary=True),\n",
       " Function(name='hash', description=None, className='org.apache.spark.sql.catalyst.expressions.Murmur3Hash', isTemporary=True),\n",
       " Function(name='hex', description=None, className='org.apache.spark.sql.catalyst.expressions.Hex', isTemporary=True),\n",
       " Function(name='hour', description=None, className='org.apache.spark.sql.catalyst.expressions.Hour', isTemporary=True),\n",
       " Function(name='hypot', description=None, className='org.apache.spark.sql.catalyst.expressions.Hypot', isTemporary=True),\n",
       " Function(name='if', description=None, className='org.apache.spark.sql.catalyst.expressions.If', isTemporary=True),\n",
       " Function(name='ifnull', description=None, className='org.apache.spark.sql.catalyst.expressions.IfNull', isTemporary=True),\n",
       " Function(name='in', description=None, className='org.apache.spark.sql.catalyst.expressions.In', isTemporary=True),\n",
       " Function(name='initcap', description=None, className='org.apache.spark.sql.catalyst.expressions.InitCap', isTemporary=True),\n",
       " Function(name='inline', description=None, className='org.apache.spark.sql.catalyst.expressions.Inline', isTemporary=True),\n",
       " Function(name='inline_outer', description=None, className='org.apache.spark.sql.catalyst.expressions.Inline', isTemporary=True),\n",
       " Function(name='input_file_block_length', description=None, className='org.apache.spark.sql.catalyst.expressions.InputFileBlockLength', isTemporary=True),\n",
       " Function(name='input_file_block_start', description=None, className='org.apache.spark.sql.catalyst.expressions.InputFileBlockStart', isTemporary=True),\n",
       " Function(name='input_file_name', description=None, className='org.apache.spark.sql.catalyst.expressions.InputFileName', isTemporary=True),\n",
       " Function(name='instr', description=None, className='org.apache.spark.sql.catalyst.expressions.StringInstr', isTemporary=True),\n",
       " Function(name='int', description=None, className='org.apache.spark.sql.catalyst.expressions.Cast', isTemporary=True),\n",
       " Function(name='isnan', description=None, className='org.apache.spark.sql.catalyst.expressions.IsNaN', isTemporary=True),\n",
       " Function(name='isnotnull', description=None, className='org.apache.spark.sql.catalyst.expressions.IsNotNull', isTemporary=True),\n",
       " Function(name='isnull', description=None, className='org.apache.spark.sql.catalyst.expressions.IsNull', isTemporary=True),\n",
       " Function(name='java_method', description=None, className='org.apache.spark.sql.catalyst.expressions.CallMethodViaReflection', isTemporary=True),\n",
       " Function(name='json_tuple', description=None, className='org.apache.spark.sql.catalyst.expressions.JsonTuple', isTemporary=True),\n",
       " Function(name='kurtosis', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.Kurtosis', isTemporary=True),\n",
       " Function(name='lag', description=None, className='org.apache.spark.sql.catalyst.expressions.Lag', isTemporary=True),\n",
       " Function(name='last', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.Last', isTemporary=True),\n",
       " Function(name='last_day', description=None, className='org.apache.spark.sql.catalyst.expressions.LastDay', isTemporary=True),\n",
       " Function(name='last_value', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.Last', isTemporary=True),\n",
       " Function(name='lcase', description=None, className='org.apache.spark.sql.catalyst.expressions.Lower', isTemporary=True),\n",
       " Function(name='lead', description=None, className='org.apache.spark.sql.catalyst.expressions.Lead', isTemporary=True),\n",
       " Function(name='least', description=None, className='org.apache.spark.sql.catalyst.expressions.Least', isTemporary=True),\n",
       " Function(name='left', description=None, className='org.apache.spark.sql.catalyst.expressions.Left', isTemporary=True),\n",
       " Function(name='length', description=None, className='org.apache.spark.sql.catalyst.expressions.Length', isTemporary=True),\n",
       " Function(name='levenshtein', description=None, className='org.apache.spark.sql.catalyst.expressions.Levenshtein', isTemporary=True),\n",
       " Function(name='like', description=None, className='org.apache.spark.sql.catalyst.expressions.Like', isTemporary=True),\n",
       " Function(name='ln', description=None, className='org.apache.spark.sql.catalyst.expressions.Log', isTemporary=True),\n",
       " Function(name='locate', description=None, className='org.apache.spark.sql.catalyst.expressions.StringLocate', isTemporary=True),\n",
       " Function(name='log', description=None, className='org.apache.spark.sql.catalyst.expressions.Logarithm', isTemporary=True),\n",
       " Function(name='log10', description=None, className='org.apache.spark.sql.catalyst.expressions.Log10', isTemporary=True),\n",
       " Function(name='log1p', description=None, className='org.apache.spark.sql.catalyst.expressions.Log1p', isTemporary=True),\n",
       " Function(name='log2', description=None, className='org.apache.spark.sql.catalyst.expressions.Log2', isTemporary=True),\n",
       " Function(name='lower', description=None, className='org.apache.spark.sql.catalyst.expressions.Lower', isTemporary=True),\n",
       " Function(name='lpad', description=None, className='org.apache.spark.sql.catalyst.expressions.StringLPad', isTemporary=True),\n",
       " Function(name='ltrim', description=None, className='org.apache.spark.sql.catalyst.expressions.StringTrimLeft', isTemporary=True),\n",
       " Function(name='map', description=None, className='org.apache.spark.sql.catalyst.expressions.CreateMap', isTemporary=True),\n",
       " Function(name='map_concat', description=None, className='org.apache.spark.sql.catalyst.expressions.MapConcat', isTemporary=True),\n",
       " Function(name='map_from_arrays', description=None, className='org.apache.spark.sql.catalyst.expressions.MapFromArrays', isTemporary=True),\n",
       " Function(name='map_from_entries', description=None, className='org.apache.spark.sql.catalyst.expressions.MapFromEntries', isTemporary=True),\n",
       " Function(name='map_keys', description=None, className='org.apache.spark.sql.catalyst.expressions.MapKeys', isTemporary=True),\n",
       " Function(name='map_values', description=None, className='org.apache.spark.sql.catalyst.expressions.MapValues', isTemporary=True),\n",
       " Function(name='max', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.Max', isTemporary=True),\n",
       " Function(name='md5', description=None, className='org.apache.spark.sql.catalyst.expressions.Md5', isTemporary=True),\n",
       " Function(name='mean', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.Average', isTemporary=True),\n",
       " Function(name='min', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.Min', isTemporary=True),\n",
       " Function(name='minute', description=None, className='org.apache.spark.sql.catalyst.expressions.Minute', isTemporary=True),\n",
       " Function(name='mod', description=None, className='org.apache.spark.sql.catalyst.expressions.Remainder', isTemporary=True),\n",
       " Function(name='monotonically_increasing_id', description=None, className='org.apache.spark.sql.catalyst.expressions.MonotonicallyIncreasingID', isTemporary=True),\n",
       " Function(name='month', description=None, className='org.apache.spark.sql.catalyst.expressions.Month', isTemporary=True),\n",
       " Function(name='months_between', description=None, className='org.apache.spark.sql.catalyst.expressions.MonthsBetween', isTemporary=True),\n",
       " Function(name='named_struct', description=None, className='org.apache.spark.sql.catalyst.expressions.CreateNamedStruct', isTemporary=True),\n",
       " Function(name='nanvl', description=None, className='org.apache.spark.sql.catalyst.expressions.NaNvl', isTemporary=True),\n",
       " Function(name='negative', description=None, className='org.apache.spark.sql.catalyst.expressions.UnaryMinus', isTemporary=True),\n",
       " Function(name='next_day', description=None, className='org.apache.spark.sql.catalyst.expressions.NextDay', isTemporary=True),\n",
       " Function(name='not', description=None, className='org.apache.spark.sql.catalyst.expressions.Not', isTemporary=True),\n",
       " Function(name='now', description=None, className='org.apache.spark.sql.catalyst.expressions.CurrentTimestamp', isTemporary=True),\n",
       " Function(name='ntile', description=None, className='org.apache.spark.sql.catalyst.expressions.NTile', isTemporary=True),\n",
       " Function(name='nullif', description=None, className='org.apache.spark.sql.catalyst.expressions.NullIf', isTemporary=True),\n",
       " Function(name='nvl', description=None, className='org.apache.spark.sql.catalyst.expressions.Nvl', isTemporary=True),\n",
       " Function(name='nvl2', description=None, className='org.apache.spark.sql.catalyst.expressions.Nvl2', isTemporary=True),\n",
       " Function(name='octet_length', description=None, className='org.apache.spark.sql.catalyst.expressions.OctetLength', isTemporary=True),\n",
       " Function(name='or', description=None, className='org.apache.spark.sql.catalyst.expressions.Or', isTemporary=True),\n",
       " Function(name='parse_url', description=None, className='org.apache.spark.sql.catalyst.expressions.ParseUrl', isTemporary=True),\n",
       " Function(name='percent_rank', description=None, className='org.apache.spark.sql.catalyst.expressions.PercentRank', isTemporary=True),\n",
       " Function(name='percentile', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.Percentile', isTemporary=True),\n",
       " Function(name='percentile_approx', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.ApproximatePercentile', isTemporary=True),\n",
       " Function(name='pi', description=None, className='org.apache.spark.sql.catalyst.expressions.Pi', isTemporary=True),\n",
       " Function(name='pmod', description=None, className='org.apache.spark.sql.catalyst.expressions.Pmod', isTemporary=True),\n",
       " Function(name='posexplode', description=None, className='org.apache.spark.sql.catalyst.expressions.PosExplode', isTemporary=True),\n",
       " Function(name='posexplode_outer', description=None, className='org.apache.spark.sql.catalyst.expressions.PosExplode', isTemporary=True),\n",
       " Function(name='position', description=None, className='org.apache.spark.sql.catalyst.expressions.StringLocate', isTemporary=True),\n",
       " Function(name='positive', description=None, className='org.apache.spark.sql.catalyst.expressions.UnaryPositive', isTemporary=True),\n",
       " Function(name='pow', description=None, className='org.apache.spark.sql.catalyst.expressions.Pow', isTemporary=True),\n",
       " Function(name='power', description=None, className='org.apache.spark.sql.catalyst.expressions.Pow', isTemporary=True),\n",
       " Function(name='printf', description=None, className='org.apache.spark.sql.catalyst.expressions.FormatString', isTemporary=True),\n",
       " Function(name='quarter', description=None, className='org.apache.spark.sql.catalyst.expressions.Quarter', isTemporary=True),\n",
       " Function(name='radians', description=None, className='org.apache.spark.sql.catalyst.expressions.ToRadians', isTemporary=True),\n",
       " Function(name='rand', description=None, className='org.apache.spark.sql.catalyst.expressions.Rand', isTemporary=True),\n",
       " Function(name='randn', description=None, className='org.apache.spark.sql.catalyst.expressions.Randn', isTemporary=True),\n",
       " Function(name='rank', description=None, className='org.apache.spark.sql.catalyst.expressions.Rank', isTemporary=True),\n",
       " Function(name='reflect', description=None, className='org.apache.spark.sql.catalyst.expressions.CallMethodViaReflection', isTemporary=True),\n",
       " Function(name='regexp_extract', description=None, className='org.apache.spark.sql.catalyst.expressions.RegExpExtract', isTemporary=True),\n",
       " Function(name='regexp_replace', description=None, className='org.apache.spark.sql.catalyst.expressions.RegExpReplace', isTemporary=True),\n",
       " Function(name='repeat', description=None, className='org.apache.spark.sql.catalyst.expressions.StringRepeat', isTemporary=True),\n",
       " Function(name='replace', description=None, className='org.apache.spark.sql.catalyst.expressions.StringReplace', isTemporary=True),\n",
       " Function(name='reverse', description=None, className='org.apache.spark.sql.catalyst.expressions.Reverse', isTemporary=True),\n",
       " Function(name='right', description=None, className='org.apache.spark.sql.catalyst.expressions.Right', isTemporary=True),\n",
       " Function(name='rint', description=None, className='org.apache.spark.sql.catalyst.expressions.Rint', isTemporary=True),\n",
       " Function(name='rlike', description=None, className='org.apache.spark.sql.catalyst.expressions.RLike', isTemporary=True),\n",
       " Function(name='rollup', description=None, className='org.apache.spark.sql.catalyst.expressions.Rollup', isTemporary=True),\n",
       " Function(name='round', description=None, className='org.apache.spark.sql.catalyst.expressions.Round', isTemporary=True),\n",
       " Function(name='row_number', description=None, className='org.apache.spark.sql.catalyst.expressions.RowNumber', isTemporary=True),\n",
       " Function(name='rpad', description=None, className='org.apache.spark.sql.catalyst.expressions.StringRPad', isTemporary=True),\n",
       " Function(name='rtrim', description=None, className='org.apache.spark.sql.catalyst.expressions.StringTrimRight', isTemporary=True),\n",
       " Function(name='schema_of_json', description=None, className='org.apache.spark.sql.catalyst.expressions.SchemaOfJson', isTemporary=True),\n",
       " Function(name='second', description=None, className='org.apache.spark.sql.catalyst.expressions.Second', isTemporary=True),\n",
       " Function(name='sentences', description=None, className='org.apache.spark.sql.catalyst.expressions.Sentences', isTemporary=True),\n",
       " Function(name='sequence', description=None, className='org.apache.spark.sql.catalyst.expressions.Sequence', isTemporary=True),\n",
       " Function(name='sha', description=None, className='org.apache.spark.sql.catalyst.expressions.Sha1', isTemporary=True),\n",
       " Function(name='sha1', description=None, className='org.apache.spark.sql.catalyst.expressions.Sha1', isTemporary=True),\n",
       " Function(name='sha2', description=None, className='org.apache.spark.sql.catalyst.expressions.Sha2', isTemporary=True),\n",
       " Function(name='shiftleft', description=None, className='org.apache.spark.sql.catalyst.expressions.ShiftLeft', isTemporary=True),\n",
       " Function(name='shiftright', description=None, className='org.apache.spark.sql.catalyst.expressions.ShiftRight', isTemporary=True),\n",
       " Function(name='shiftrightunsigned', description=None, className='org.apache.spark.sql.catalyst.expressions.ShiftRightUnsigned', isTemporary=True),\n",
       " Function(name='shuffle', description=None, className='org.apache.spark.sql.catalyst.expressions.Shuffle', isTemporary=True),\n",
       " Function(name='sign', description=None, className='org.apache.spark.sql.catalyst.expressions.Signum', isTemporary=True),\n",
       " Function(name='signum', description=None, className='org.apache.spark.sql.catalyst.expressions.Signum', isTemporary=True),\n",
       " Function(name='sin', description=None, className='org.apache.spark.sql.catalyst.expressions.Sin', isTemporary=True),\n",
       " Function(name='sinh', description=None, className='org.apache.spark.sql.catalyst.expressions.Sinh', isTemporary=True),\n",
       " Function(name='size', description=None, className='org.apache.spark.sql.catalyst.expressions.Size', isTemporary=True),\n",
       " Function(name='skewness', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.Skewness', isTemporary=True),\n",
       " Function(name='slice', description=None, className='org.apache.spark.sql.catalyst.expressions.Slice', isTemporary=True),\n",
       " Function(name='smallint', description=None, className='org.apache.spark.sql.catalyst.expressions.Cast', isTemporary=True),\n",
       " Function(name='sort_array', description=None, className='org.apache.spark.sql.catalyst.expressions.SortArray', isTemporary=True),\n",
       " Function(name='soundex', description=None, className='org.apache.spark.sql.catalyst.expressions.SoundEx', isTemporary=True),\n",
       " Function(name='space', description=None, className='org.apache.spark.sql.catalyst.expressions.StringSpace', isTemporary=True),\n",
       " Function(name='spark_partition_id', description=None, className='org.apache.spark.sql.catalyst.expressions.SparkPartitionID', isTemporary=True),\n",
       " Function(name='split', description=None, className='org.apache.spark.sql.catalyst.expressions.StringSplit', isTemporary=True),\n",
       " Function(name='sqrt', description=None, className='org.apache.spark.sql.catalyst.expressions.Sqrt', isTemporary=True),\n",
       " Function(name='stack', description=None, className='org.apache.spark.sql.catalyst.expressions.Stack', isTemporary=True),\n",
       " Function(name='std', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.StddevSamp', isTemporary=True),\n",
       " Function(name='stddev', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.StddevSamp', isTemporary=True),\n",
       " Function(name='stddev_pop', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.StddevPop', isTemporary=True),\n",
       " Function(name='stddev_samp', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.StddevSamp', isTemporary=True),\n",
       " Function(name='str_to_map', description=None, className='org.apache.spark.sql.catalyst.expressions.StringToMap', isTemporary=True),\n",
       " Function(name='string', description=None, className='org.apache.spark.sql.catalyst.expressions.Cast', isTemporary=True),\n",
       " Function(name='struct', description=None, className='org.apache.spark.sql.catalyst.expressions.NamedStruct', isTemporary=True),\n",
       " Function(name='substr', description=None, className='org.apache.spark.sql.catalyst.expressions.Substring', isTemporary=True),\n",
       " Function(name='substring', description=None, className='org.apache.spark.sql.catalyst.expressions.Substring', isTemporary=True),\n",
       " Function(name='substring_index', description=None, className='org.apache.spark.sql.catalyst.expressions.SubstringIndex', isTemporary=True),\n",
       " Function(name='sum', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.Sum', isTemporary=True),\n",
       " Function(name='tan', description=None, className='org.apache.spark.sql.catalyst.expressions.Tan', isTemporary=True),\n",
       " Function(name='tanh', description=None, className='org.apache.spark.sql.catalyst.expressions.Tanh', isTemporary=True),\n",
       " Function(name='timestamp', description=None, className='org.apache.spark.sql.catalyst.expressions.Cast', isTemporary=True),\n",
       " Function(name='tinyint', description=None, className='org.apache.spark.sql.catalyst.expressions.Cast', isTemporary=True),\n",
       " Function(name='to_date', description=None, className='org.apache.spark.sql.catalyst.expressions.ParseToDate', isTemporary=True),\n",
       " Function(name='to_json', description=None, className='org.apache.spark.sql.catalyst.expressions.StructsToJson', isTemporary=True),\n",
       " Function(name='to_timestamp', description=None, className='org.apache.spark.sql.catalyst.expressions.ParseToTimestamp', isTemporary=True),\n",
       " Function(name='to_unix_timestamp', description=None, className='org.apache.spark.sql.catalyst.expressions.ToUnixTimestamp', isTemporary=True),\n",
       " Function(name='to_utc_timestamp', description=None, className='org.apache.spark.sql.catalyst.expressions.ToUTCTimestamp', isTemporary=True),\n",
       " Function(name='transform', description=None, className='org.apache.spark.sql.catalyst.expressions.ArrayTransform', isTemporary=True),\n",
       " Function(name='translate', description=None, className='org.apache.spark.sql.catalyst.expressions.StringTranslate', isTemporary=True),\n",
       " Function(name='trim', description=None, className='org.apache.spark.sql.catalyst.expressions.StringTrim', isTemporary=True),\n",
       " Function(name='trunc', description=None, className='org.apache.spark.sql.catalyst.expressions.TruncDate', isTemporary=True),\n",
       " Function(name='ucase', description=None, className='org.apache.spark.sql.catalyst.expressions.Upper', isTemporary=True),\n",
       " Function(name='unbase64', description=None, className='org.apache.spark.sql.catalyst.expressions.UnBase64', isTemporary=True),\n",
       " Function(name='unhex', description=None, className='org.apache.spark.sql.catalyst.expressions.Unhex', isTemporary=True),\n",
       " Function(name='unix_timestamp', description=None, className='org.apache.spark.sql.catalyst.expressions.UnixTimestamp', isTemporary=True),\n",
       " Function(name='upper', description=None, className='org.apache.spark.sql.catalyst.expressions.Upper', isTemporary=True),\n",
       " Function(name='uuid', description=None, className='org.apache.spark.sql.catalyst.expressions.Uuid', isTemporary=True),\n",
       " Function(name='var_pop', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.VariancePop', isTemporary=True),\n",
       " Function(name='var_samp', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.VarianceSamp', isTemporary=True),\n",
       " Function(name='variance', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.VarianceSamp', isTemporary=True),\n",
       " Function(name='weekday', description=None, className='org.apache.spark.sql.catalyst.expressions.WeekDay', isTemporary=True),\n",
       " Function(name='weekofyear', description=None, className='org.apache.spark.sql.catalyst.expressions.WeekOfYear', isTemporary=True),\n",
       " Function(name='when', description=None, className='org.apache.spark.sql.catalyst.expressions.CaseWhen', isTemporary=True),\n",
       " Function(name='window', description=None, className='org.apache.spark.sql.catalyst.expressions.TimeWindow', isTemporary=True),\n",
       " Function(name='xpath', description=None, className='org.apache.spark.sql.catalyst.expressions.xml.XPathList', isTemporary=True),\n",
       " Function(name='xpath_boolean', description=None, className='org.apache.spark.sql.catalyst.expressions.xml.XPathBoolean', isTemporary=True),\n",
       " Function(name='xpath_double', description=None, className='org.apache.spark.sql.catalyst.expressions.xml.XPathDouble', isTemporary=True),\n",
       " Function(name='xpath_float', description=None, className='org.apache.spark.sql.catalyst.expressions.xml.XPathFloat', isTemporary=True),\n",
       " Function(name='xpath_int', description=None, className='org.apache.spark.sql.catalyst.expressions.xml.XPathInt', isTemporary=True),\n",
       " Function(name='xpath_long', description=None, className='org.apache.spark.sql.catalyst.expressions.xml.XPathLong', isTemporary=True),\n",
       " Function(name='xpath_number', description=None, className='org.apache.spark.sql.catalyst.expressions.xml.XPathDouble', isTemporary=True),\n",
       " Function(name='xpath_short', description=None, className='org.apache.spark.sql.catalyst.expressions.xml.XPathShort', isTemporary=True),\n",
       " Function(name='xpath_string', description=None, className='org.apache.spark.sql.catalyst.expressions.xml.XPathString', isTemporary=True),\n",
       " Function(name='year', description=None, className='org.apache.spark.sql.catalyst.expressions.Year', isTemporary=True),\n",
       " Function(name='zip_with', description=None, className='org.apache.spark.sql.catalyst.expressions.ZipWith', isTemporary=True),\n",
       " Function(name='|', description=None, className='org.apache.spark.sql.catalyst.expressions.BitwiseOr', isTemporary=True),\n",
       " Function(name='~', description=None, className='org.apache.spark.sql.catalyst.expressions.BitwiseNot', isTemporary=True)]"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listFunctions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **currentDatabase()**\n",
    "\n",
    "Get the current DataBase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'default'"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.currentDatabase()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **setCurrentDatabase()**\n",
    "\n",
    "Set the Current Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Un-comment below code and specify the DB_Name to be set as current Database.\n",
    "#spark.catalog.setCurrentDatabase(<DB_Name>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **cacheTable()**\n",
    "\n",
    "cache a particular table.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.catalog.cacheTable(\"default.hive_empdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **isCached()**\n",
    "\n",
    "Check if Table is cached or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.isCached(\"default.hive_empdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **uncacheTable()**\n",
    "\n",
    "Un-cache particular table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.catalog.uncacheTable(\"default.hive_empdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verify uncached table. Now you will see that it will return \"False\" which means table is not cached.\n",
    "spark.catalog.isCached(\"default.hive_empdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **clearCache()**\n",
    "\n",
    "Un-cache all table in Spark session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.catalog.clearCache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# THANK You - End of Part 3"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
